syntax = "proto2";

package yolov3;

message Train {
    required string dataset = 1;
    required int32 epochs = 2;
    required int32 batch_size = 3;
    required Optimizer optimizer_config = 4;
    optional string fine_tune_checkpoint = 5 [default = ""];
    required string log = 6;
}

message TrainConfig {
    required Train train_config = 1;
}

message Optimizer {
    oneof optimizer {
        AdaDeltaOptimizer adadelta_optimizer = 1;
        AdaGradOptimizer adagrad_optimizer = 2;
        AdamOptimizer adam_optimizer = 3;
        NadamOptimizer nadam_optimizer = 4;
        RMSPropOptimizer rmsprop_optimizer = 5;
        SGDOptimizier sgd_optimizer = 6;
    }
    oneof learning_rate {
        ExponentialDecayLR exponential_decay = 7;
        InverseTimeDecayLR inversetime_decay = 8;
        PieceWiseConstantDecayLR piecewise_constant_decay = 9;
        PolynomialDecayLR polynomial_decay = 10;
        ConstantLR constant = 11;
    }
}

message AdaDeltaOptimizer {
    optional float learning_rate = 1 [default = 0.001];
    optional float rho = 2 [default = 0.95];
    optional float epsilon = 3 [default = 1e-07];
}

message AdaGradOptimizer {
    optional float learning_rate = 1 [default = 0.001];
    optional float initial_accumulator_value = 2 [default = 0.1];
    optional float epsilon = 3 [default = 1e-07];
}

message AdamOptimizer {
    optional float learning_rate = 1 [default = 0.001];
    optional float beta_1 = 2 [default = 0.9];
    optional float beta_2 = 3 [default = 0.999];
    optional float epsilon = 4 [default = 1e-07];
    optional bool amsgrad = 5 [default = false];
}

message AdamaxOptimizer {
    optional float learning_rate = 1 [default = 0.001];
    optional float beta_1 = 2 [default = 0.9];
    optional float beta_2 = 3 [default = 0.999];
    optional float epsilon = 4 [default = 1e-07];
}

message NadamOptimizer {
    optional float learning_rate = 1 [default = 0.001];
    optional float beta_1 = 2 [default = 0.9];
    optional float beta_2 = 3 [default = 0.999];
    optional float epsilon = 4 [default = 1e-07];
}

message RMSPropOptimizer {
    optional float learning_rate = 1 [default = 0.001];
    optional float rho = 2 [default = 0.9];
    optional float momentum = 3 [default = 0.0];
    optional float epsilon = 4 [default = 1e-07];
    optional bool centered = 5 [default = false];
}

message SGDOptimizier {
    optional float learning_rate = 1 [default = 0.001];
    optional float momentum = 2 [default = 0.0];
    optional bool nesterov = 3 [default = false];
}

message ExponentialDecayLR {
    required float initial_learning_rate = 1;
    required int32 decay_steps = 2;
    required float decay_rate = 3;
    optional bool staircase = 4 [default = false];
}

message InverseTimeDecayLR {
    required float initial_learning_rate = 1;
    required int32 decay_steps = 2;
    required float decay_rate = 3;
    optional bool staircase = 4 [default = false];
}

// number of boundaries values should be 1 less than 
// the number of values. For example,
//
// boundaries = [100, 200]
// values = [0.1, 0.01, 0.001]
//
// lr = PiecewiseConstantDecay(boundaries, values)
// lr(0) -> 0.1
// lr(99) -> 0.1
// lr(101) -> 0.01
// lr(199) -> 0.01
// lr(200) -> 0.001
// lr(100000) -> 0.001 
message PieceWiseConstantDecayLR {
    repeated int32 boundaries = 1;
    repeated float values = 2;
}

message PolynomialDecayLR {
    required float initial_learning_rate = 1;
    required int32 decay_steps = 2;
    optional float end_learning_rate = 3 [default = 0.0001];
    optional float power = 4 [default = 1.0];
    optional bool cycle = 5 [default = false];
}

message ConstantLR {
    optional float learning_rate = 1 [default = 0.001];
}
